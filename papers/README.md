# Downloaded Papers

1. [Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench](humorbench_2507.21476v1_Which_LLMs_Get_the_Joke_Probing_NonSTEM_Reasoning_.pdf)
   - **Relevance**: Introduces a benchmark for evaluating humor reasoning in LLMs.
   - **Key Concepts**: Humor recognition, benchmarking, non-STEM reasoning.

2. [Linear Representations of Sentiment in Large Language Models](linear_sentiment_2310.15154v1_Linear_Representations_of_Sentiment_in_Large_Langu.pdf)
   - **Relevance**: Methodological reference for finding linear representations (directions) in LLM activation space.
   - **Key Concepts**: Linear probes, sentiment direction, activation space geometry.

3. [UR-FUNNY: A Multimodal Language Dataset for Understanding Humor](understanding_humor_1904.06618v1_URFUNNY_A_Multimodal_Language_Dataset_for_Understa.pdf)
   - **Relevance**: Provides a dataset for humor detection (though multimodal, text part is useful).
   - **Key Concepts**: Humor detection, dataset, multimodal.

4. [LoRA: Low-Rank Adaptation of Large Language Models](lora_2106.09685v2_LoRA_LowRank_Adaptation_of_Large_Language_Models.pdf)
   - **Relevance**: Establishes the concept of "low-rank" updates in LLMs, relevant to the hypothesis of low-rank representations.
   - **Key Concepts**: Low-rank matrices, adaptation, efficiency.

5. [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](intrinsic_dim_2012.13255v1_Intrinsic_Dimensionality_Explains_the_Effectivenes.pdf)
   - **Relevance**: Discusses the intrinsic dimension of LLM tasks, supporting the "low rank" hypothesis.
   - **Key Concepts**: Intrinsic dimension, subspaces, fine-tuning.
